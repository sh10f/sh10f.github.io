---
layout: post
title: ML2025-HongyiLi-HW3-Understanding Transformer
date: 2025-04-04 16:00:00
summary: 
categories: ML2025-Hongyi_Li
---

* 实验采用 Model 如下：
  * `google/gemma-2-2b-it`
  * ms-macro-MiniLM-L6-v2
* 有趣的点
  * <a href="#chat template">Why the Coherence Score with Chat Template is higher than Without template？</a>
  * 

# Problem 1 - Chat template Comparison (1pt) 

Task **Descriptions**: Observations of response with/without chat template. 

**Prompt**: “Please tell me about the key differences between supervised learning and unsupervised learning. Answer in 200 words.” 

**Questions**: Calculate and compare the coherence score between responses generated with and without the chat template. 

## 1   What is each coherence score? 

* with chat template
  * ![image-20250403141840838](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403141840838.png)
  * Coherence Score： 4.3467
* Without Chat Template
  * ![image-20250403141941632](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403141941632.png)
  * Coherence Score：-3.9188



## 2. Which score is higher?

- the Score **with Chat Template** is **higher**



## 3 Why the Coherence Score with Chat Template is higher than Without template？

<a name="chat template"> 🤔</a>

- 最开始我以为是 **Prompt Engineer** 导致的区别，即 提示词最好是结构化数据

  - 例如

    ```python
    Prompt = """
    ### 角色配置：你是一个专业的数据分析师
    ### 任务说明：请你使用Numpy和Pandas对给定数据进行分析
    ### 示例：
    # 示例1：XXXX
    ### 数据如下：
    # 数据： XXXXX
    ### 格式要求：
    # 请你输出时在 “【分析】”后面进行
    """
    ```

  - 实验结果为：

    ![image-20250403163344701](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403163344701.png)

  - Coherence Score： 2.57

  - 通过单纯提示词没有达到Chat Template的效果，说明还有其他原因

- 查询huggingface文档([Chat Templates](https://huggingface.co/learn/nlp-course/chapter11/2?fw=pt)) 发现有 Base Model 和 Instruct Model 之分

  - Base Model 是在原始数据上进行训练
  - Instruct Mode是经过微调后的模型，指令跟随的能力更强。但是这也要求了使用对应的Chat Template，才能激发LLM的能力

- Example： `SmolLM2-135M-Instruct` 的 chat Template configuration

  - ```jinja2
    "bos_token": "<|im_start|>",
    
    "chat_template": "
    {% for message in messages %}
        {% if loop.first and messages[0]['role'] != 'system' %}
            {{ '<|im_start|>system\nYou are a helpful AI assistant named SmolLM...<|im_end|>\n' }}
        {% endif %} // 如果对话第一条消息不是 system 角色，自动插入一条默认提示
        {{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}} // 每条message按照以上格式进行包装
    {% endfor %}
    {% if add_generation_prompt %}
        {{ '<|im_start|>assistant\n' }} // 在对话末尾添加标记，提示模型开始生成回复。
    {% endif %},
    
    "clean_up_tokenization_spaces": false,
    
    "eos_token": "<|im_end|>",
    ```

    

- 更换  [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it/blob/main/tokenizer_config.json) 对应模板后：

  - ![image-20250403174534939](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403174534939.png)
  - Coherence Score：7.22
  - 返回的content包含了user的内容，但是model content和使用chat Template的输出一致

- **difference** between Chat Templates

  - **System Message handling**
    - 不同的模型对 System Message的处理不同
    - 例如 `gemma-2-2b-it` 会将 `system`替换为 `model`
    - 包含 `system` 的 ***tag*** 不同。 `Qwen`采用 `<|im_start|>`, `GPT` 采用 `SYSTEM`
  - **Message Tag**

- 为了 自适应这些 不同的 Chat Templates

  -  `transformer`库中 `AutoTokenizer.from_pretrained` 会加载 `chat_template`属性。
  - 后面在使用 `tokenizer.apply_chat_template(messages, tokenize=False)`时自动修改Message。

* ***Guidelines***

  * 一致的模板格式

  * 清晰定义 Role。 常常包括 `system，user，assistant，tool`

  * 上下文记忆管理。警惕超出 Token Limits

  * Example：

    * ```json
      messages = [
          {
              "role": "system",
              "content": "You are a helpful vision assistant that can analyze images.",
          },
          {
              "role": "user",
              "content": [
                  {"type": "text", "text": "What's in this image?"},
                  {"type": "image", "image_url": "https://example.com/image.jpg"},
              ],
          },
      ]
      ```

      



# Problem 2 - Multi-turn Conversations

Task **Descriptions**: Observe the response from the following multi-turn conversation. You should check the possibility of the model response and the format of the prompt inputted to the model. 



**Conversation History:** 

```python
{
    User (Your 1st Input): “Name a color in a rainbow, please just answer in a word without any emoji.” 
    Model 1st output : xxxx. 
    
    User (Your 2nd Input): “That’s great! Now, could you tell me another color that I can find in a rainbow?” 
    Model 2nd output : xxxx. 
    
    User (Your 3rd Input): “Could you continue and name yet another color from the rainbow?” 
    Model 3rd output : xxxx. }
```



## 1  Provide the correct FULL prompt with chat template format for the third round.

```jinja2
<bos><start_of_turn>user
Name a color in a rainbow, please just answer in a word without any emoji.<end_of_turn>
<start_of_turn>model
Indigo<end_of_turn>
<start_of_turn>user
That’s great! Now, could you tell me another color that I can find in a rainbow?<end_of_turn>
<start_of_turn>model
Orange<end_of_turn>
<start_of_turn>user
Could you continue and name yet another color from the rainbow?<end_of_turn>
<start_of_turn>model
```

* 由于 过去记忆 (round 1) 的存在，导致 Round 3 输出也是 一个word
* 



## 2 What is the first token with the highest probability in the first round (question)?

![image-20250403191043056](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403191043056.png)

* 第一轮的第一个 Token 是 `<h1>`



## 3 How to check Tokens when generatint？

```python
inputs = tokenizer(chat_template_format_prompt, return_tensors="pt").to("cuda")

# Get logits instead of directly generating
with torch.no_grad():
    outputs_p = model(**inputs)

logits = outputs_p.logits  # Logits of the model (raw scores before softmax)
print("logits shape: ", logits.shape)

last_token_logits = logits[:, 0, :]  # Take the logits of the first generated token (1, 5, 25600)

# Apply softmax to get probabilities  (1, 25600)
probs = torch.nn.functional.softmax(last_token_logits, dim=-1)
print("prob shape: ", probs.shape)

# Get top-k tokens (e.g., 10)
top_k = 10
top_probs, top_indices = torch.topk(probs, top_k)

# Convert to numpy for plotting
top_probs = top_probs.cpu().squeeze().numpy()
top_indices = top_indices.cpu().squeeze().numpy()
top_tokens = [tokenizer.decode([idx]) for idx in top_indices]
```





# Problem 3 - tokenization of a sentence

**Prompt**: "I love taking a Machine Learning course by Professor Hung-yi Lee, What about you?" 



## 1 How is the prompt being tokenized into?

**Fill-in-the-blank Questions**: How is the prompt being tokenized into? Please write the corresponding token index. 

<img src="https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403192007150.png" alt="image-20250403192007150" style="zoom:50%;" />

```json
Token: <bos>, token index: 2

Token: I, token index: 235285

Token:  love, token index: 2182

Token:  taking, token index: 4998

Token:  a, token index: 476

Token:  Machine, token index: 13403

Token:  Learning, token index: 14715

Token:  course, token index: 3205

Token:  by, token index: 731

Token:  Professor, token index: 11325

Token:  Hung, token index: 18809

Token: -, token index: 235290

Token: yi, token index: 12636

Token:  Lee, token index: 9201

Token: ,, token index: 235269

Token:  What, token index: 2439

Token:  about, token index: 1105

Token:  you, token index: 692

Token: ?, token index: 235336
```



## 2. Tokenization

* *Word-based Tokenizer*

  * space
  * punctuation

  离散表示。根据corpus建立词汇表，加上一个 `<UNKNOWN>` .

  缺点是 ：

  - 语义相似的词汇，在表示上却体现不出语义相似，因为是稀疏one-hot向量，它们不相似
  - 向量表示 维度过大
  - 对于不在词汇表中词，统一表示为 `<UNKNOWN>`, 信息缺失很多

* *Character-based Tokenizer*

  * 词汇表变小
  * Unknown变少
  * ![image-20250403200329985](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403200329985.png)
  * 缺点是：丢失了部分信息，Character不像word一样有价值。不同语言的Character的语义含量不同

* *Subword-based Tokenizer*

  * 类似 哈夫曼树，使用频率高的词语单独是一个Token， 频率低的词语会分解成频率高的Token
  * ![image-20250403203233954](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250403203233954.png)
  * 优点是：
    * 更小的词汇表，只需要 使用频率高的Tokens
    * 陌生的词汇可以拆分成使用频率高的词汇

* ***Text To Sequence***

  * Text to Tokens：

    * ```python
      sequence = "Using a Transformer network is simple"
      tokens = tokenizer.tokenize(sequence)
      # ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
      ```

    * `Tokenize` 将word不断分割，直到词汇表能够表示，这也造成了 `transformer` 被分割为 `transform` 和 `##er`

  * Tokens to IDs

    * ```python
      ids = tokenizer.convert_tokens_to_ids(tokens)
      # [7993, 170, 11303, 1200, 2443, 1110, 3014]
      ```

  * IDs back to Text

    * ```python
      decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
      # 'Using a Transformer network is simple'
      ```

    * `decode` 方法不仅将 IDs 转换为 Tokens， 还将同一个word的Tokens组合为一个Word


## Summary of Tokenization

* LLM的输入是离散的IDs序列
* Text To IDs序列 需要 **两步**
  * Text To Tokens
  * Tokens To IDs
* 为什么我们使用Pre-Embedding的结果作为LLM的输入，这是因为
  * LLM在训练过程中 Embedding 的权重是随着训练更新的，这样Embedding 更加适配训练任务和数据，不会固定




# Problem 4 - Autoregressive Generation



Task **Descriptions**: 

*  Use auto-regressive generation to generate a sentence 20 times. 

* Calculate the **self-BLEU** score for the 20 sentences. 

- Compare Top-k sampling (k=2) vs. Top-k sampling (k=200) 

- Compare Top-p sampling (p=0.6) vs Top-p sampling (p=0.999) 

- Observe fluency, coherence, and diversity.

  

**Prompt**: "Generate a paraphrase of the sentence 'Professor Lee is one of the best 
teachers in the domain of machine learning'. Just response with one sentence."



## 1 What about self-BLEU score? 

* **Self-BLEU** 基于 **BLEU** (BiLingual Evaluation Understudy)
  * Self-BLEU 是生成样本集中每个样本针对其余生成样本的 BLEU 平均
  * Self-BLEU是测量 生成样本集的多样性(diversity)的。Self-BLEU越小，多样性越好
* **BLEU** 基于 **n-gram**
  * BLEU 是 两个样本的1-gram 到 n-gram 中 匹配概率的 几何平均
  * BLEU 是测量 两个样本的相似度的。NLEU越大，相似度越大



## 2 What about top-p and top-k?

*  **Top-K，Top-P**是不同的随机采样策略。还可以通过 **Temperature** 机制调节概率分布

* Top-K是在前 K 个概率最大的样本中进行 Softmax。即在这 前 K 个 样本中采样

  * 优点：随机采样
  * 缺点：
    * 固定候选集，个数为K
    * 受到样本概率分布的影响较大。若候选词方差较大，可以采样到概率极小的候选词，导致"胡言乱语"。若方差均匀，则采样到的都是概率相近的候选，多样性较差

* Top-P 是在概率之和 **大于** P的样本集合中采样

  * 候选集数量不固定。
  * 一定程度避免概率分布的影响。方差较大，可能概率大的候选词的概率和就超过了P，这样就避免了选择概率极小的候选，不会胡言乱语。均匀分布时，也会更多的容纳候选词，增加多样性

* Temperature 机制就是在计算Softmax之前，所有样本先除以一个Temperature

  * $$
    p\left(w_{i+1}^{1}, \ldots, w_{i+1}^{K}\right)=\left\{\frac{\exp \left(\frac{o_{i}\left[w_{i+1}^{1}\right]}{T}\right)}{\sum_{j=1}^{K} \exp \left(\frac{o_{i}\left[w_{i+1}^{j}\right]}{T}\right)}, \ldots, \frac{\exp \left(\frac{o_{i}\left[w_{i+1}^{K}\right]}{T}\right)}{\sum_{j=1}^{K} \exp \left(\frac{o_{i}\left[w_{i+1}^{j}\right]}{T}\right)}\right\}
    $$

  * Temperature 一般默认为 1.

  * 若 Temperature > 1，会缩小概率之间的差距，增大了之前概率小的样本的被采样到的机会，增大了样本多样性

  * 若 Temperature < 1, 会增大概率之间的差距，多样性减小

## 3 What is the generated sentence of top-k for k = 1?

* **All identical !!**
* ![image-20250404113615724](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250404113615724.png)

## 4 What is the generated sentence of top-p for p = 0?

* **All identical !!**
* 因为第一个样本的概率 一定大于 P=0. 所以永远只会采样第一个样本
* ![image-20250404113720611](https://raw.githubusercontent.com/sh10f/images/master/img/image-20250404113720611.png)

## 5 Compare the self-BLEU score of top-k for different k values ( 2 vs 200 ), which is higher and why? 



* |           | K=2    | K=20   |
  | :-------: | ------ | ------ |
  | Self-BLEU | 0.2704 | 0.0599 |
  |           |        |        |

## 6 Compare the self-BLEU score of top-p for different p values ( 0.6 vs 0.999 )? 



* |           | P=0.6  | P = 0.999 |
  | :-------: | ------ | --------- |
  | Self-BLEU | 0.5963 | 0.0952    |
  |           |        |           |






# Problem 5 - t-SNE 

Task **Descriptions**: Plotting the t-SNE 2-D Embeddings 

**Sentences**: (Provided in sample code) 

* "I ate a fresh apple.", # Apple (fruit) 
* "Apple released the new iPhone.", # Apple (company) 
* "I peeled an orange and ate it.", # Orange (fruit) 
* "The Orange network has great coverage.", # Orange (telecom) 
* "Microsoft announced a new update.", # Microsoft (company) 
* "Banana is my favorite fruit.", # Banana (fruit)



<img src="https://raw.githubusercontent.com/sh10f/images/master/img/image-20250404135734111.png" alt="image-20250404135734111" style="zoom: 67%;" />





# Problem 6 - Attention Map

Task **Descriptions**: Plot and observe the figure of the attention map 

**Prompt**: “Google ”

 Generated **tokens**: 20 

**Layer** index (Recommended): 10 

**Head** index (Recommended): 7

<img src="https://raw.githubusercontent.com/sh10f/images/master/img/image-20250404190151064.png" alt="image-20250404190151064" style="zoom:67%;" />
